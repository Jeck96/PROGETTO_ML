# -*- coding: utf-8 -*-
"""progettoML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lpOXf5dnW9GDJiwJROyWa1VXbkrKeMKu

# Explore and wrangling data
"""

# Let's link our gdrive to colab
from google.colab import drive 
drive.mount('/content/gdrive', force_remount=True)

import pandas as pd
import numpy as np

# Let's load our raw dataset
datasetRaw = pd.read_csv('gdrive/My Drive/KaggleV2-May-2016.csv')

# Let's see general info and if there are missing value
print("The shape of our DataFrame is => {}".format(datasetRaw.shape))

datasetRaw.info()

"""Show data with pyplot

No missing value.
"""

import matplotlib.pyplot as plt
variable = 'Diabetes'
print(datasetRaw.groupby([variable,'No-show']).size()[0][0])
#dist:distribuzione
dist = datasetRaw.groupby([variable,'No-show']).size()
x_labels = [f'with {variable} ',f'without {variable}']
fig, ax  = plt.subplots()
width = 0.50
print (f'without {variable} show: {dist[0][0]}   without {variable} No-show: {dist[0][1]}    with {variable} show: {dist[1][0]} with {variable} No-show: {dist[1][1]}')

without = [dist[0][0],dist[0][1]]
with_ = [dist[1][0],dist[1][1]]
ax.bar(x_labels, without, width,color='#d62728',label ='Show')
ax.bar(x_labels, with_, width, color='blue', bottom=without, label ='No-Show')
ax.set_ylabel('Number of Patients')
ax.legend()
plt.show()

#let's see how many patients missed their appointment
print("NoShow and Show Count of Patients\n")
print(datasetRaw.groupby(['No-show']).size())

print("\nNoShow and Show '%' of Patients\n")
show = datasetRaw.groupby(['No-show']).size()[0]/(datasetRaw.groupby(['No-show']).size()[0]+datasetRaw.groupby(['No-show']).size()[1])
print("Percent of Patients who `Showed Up` => {:.2f}%".format(show*100))
noshow = datasetRaw.groupby(['No-show']).size()[1]/(datasetRaw.groupby(['No-show']).size()[0]+datasetRaw.groupby(['No-show']).size()[1])
print("Percent of Patients who Did `Not Showed Up` => {:.2f}%".format(noshow*100))

"""Around 2/3 (old 80%) of the patients are coming for the visit after an appointment and around 1/3 (old 20%) are skipping their appointments."""

# Print features names
print("Features in the DataFrame => {}".format(datasetRaw.columns.ravel()))

# Rename possible column names typos
datasetRaw = datasetRaw.rename(columns={'Hipertension': 'Hypertension', 'Handcap': 'Handicap', 'SMS_received': 'SMSReceived', 'No-show': 'NoShow'})

# Let's explore the head to see how the records look
datasetRaw.head()

# Drop 'PatientId' and 'AppointmentID' as they are just some system generated numbers
datasetRaw.drop(['PatientId', 'AppointmentID'], axis=1, inplace=True)

# Print Unique Values
print("Unique Values in `Gender` => {}".format(datasetRaw.Gender.unique()))
print("Unique Values in `Age` => {}".format(datasetRaw.Age.unique()))
print("Unique Values in `Neighbourhood` => {}".format(datasetRaw.Neighbourhood.unique()))
print("Unique Values in `Scholarship` => {}".format(datasetRaw.Scholarship.unique()))
print("Unique Values in `Hypertension` => {}".format(datasetRaw.Hypertension.unique()))
print("Unique Values in `Diabetes` => {}".format(datasetRaw.Diabetes.unique()))
print("Unique Values in `Alcoholism` => {}".format(datasetRaw.Alcoholism.unique()))
print("Unique Values in `Handicap` => {}".format(datasetRaw.Handicap.unique()))
print("Unique Values in `SMSReceived` => {}".format(datasetRaw.SMSReceived.unique()))
print("Unique Values in `NoShow` => {}".format(datasetRaw.NoShow.unique()))

"""We can begin cleaning and wrangling our data.

We can already see that there some patients that are -1 years old, we can assume the 0 years old as babies, and on the other end we have patients with an odd 115.
"""

# Let's see how many they are
print("Patients with `Age` less than -1 -> {}".format(datasetRaw[datasetRaw.Age == -1].shape[0]))
print("Patients with `Age` equal to 0 -> {}".format(datasetRaw[datasetRaw.Age == 0].shape[0]))
print("Patients with `Age` equal to 115 -> {}".format(datasetRaw[datasetRaw.Age == 115].shape[0]))

"""We can safely delete the wrong record with the patient with -1 as age and also the odd ones that are 115 years old (please note that we have a weird jump from 102 to 115 in the age field), furthermore we can keep the patients 0 years old and assume them as babies."""

# Let's remove the records with the odd age
datasetRaw=datasetRaw[(datasetRaw['Age'] >= 0) & (datasetRaw['Age'] < 115)]

"""ScheduledDay is the timestamp of the call from the patient that wants to set an appointment. 

AppointmentDay is the day of the actual scheduled visit when the patient has to see the doctor, and it misses the accurate time, we have only day/month/year.

We don't need the precise time, and in the meantime let's use an actual date object for this 2 features.
"""

# Convert ScheduledDay and AppointmentDay to datetime64[ns]
datasetRaw['ScheduledDay'] = pd.to_datetime(datasetRaw['ScheduledDay']).dt.date.astype('datetime64[ns]')
datasetRaw['AppointmentDay'] = pd.to_datetime(datasetRaw['AppointmentDay']).dt.date.astype('datetime64[ns]')

# Let's check random records to see how the change turned out
datasetRaw.sample(n=10)

"""Now let's check if maybe there are some records where the appointment day is before the schedule day."""

# How many records have this error?
a = np.asanyarray((datasetRaw['AppointmentDay'] - datasetRaw['ScheduledDay']).dt.days < 0)

print(a)

get_indexes = lambda x, xs: [i for (y, i) in zip(xs, range(len(xs))) if x == y]

print(len(get_indexes(False,a)))
print(len(get_indexes(True,a)))
print(get_indexes(True,a))

"""We have 5 records that have the appointment day before the schedule day. 

It's a very small number, we can safely remove them from the dataset.
"""

# Let's remove those records
datasetRaw = datasetRaw[(datasetRaw['AppointmentDay'] - datasetRaw['ScheduledDay']).dt.days >= 0]

# Let's see if it turned out okay
datasetRaw.info()

datasetRaw.head()

datasetRaw.sample(n=10)

a = np.asanyarray((datasetRaw['AppointmentDay'] - datasetRaw['ScheduledDay']).dt.days < 0)
print(a)
print(len(get_indexes(False,a)))
print(len(get_indexes(True,a)))
print(get_indexes(True,a))

"""It's all good.

Now let's extract some more info that could be useful like the awaiting days and the day of the week from the dates, and delete some info that could not help us like the actual complete date.
"""

# Get Day of the Week for ScheduledDay and AppointmentDay
datasetRaw['ScheduledDayofWeek'] = datasetRaw['ScheduledDay'].dt.weekday
datasetRaw['AppointmentDayofWeek'] = datasetRaw['AppointmentDay'].dt.weekday
datasetRaw['AwaitingDays']=(datasetRaw['AppointmentDay'] - datasetRaw['ScheduledDay']).dt.days
datasetRaw = datasetRaw.drop(['AppointmentDay','ScheduledDay'],axis=1)
datasetRaw.sample(n=10)

"""Now let's see how many patients missed their appointment."""

print("NoShow and Show Count of Patients\n")
print(datasetRaw.groupby(['NoShow']).size())

print("\nNoShow and Show '%' of Patients\n")
show = datasetRaw.groupby(['NoShow']).size()[0]/(datasetRaw.groupby(['NoShow']).size()[0]+datasetRaw.groupby(['NoShow']).size()[1])
print("Percent of Patients who `Showed Up` => {:.2f}%".format(show*100))
noshow = datasetRaw.groupby(['NoShow']).size()[1]/(datasetRaw.groupby(['NoShow']).size()[0]+datasetRaw.groupby(['NoShow']).size()[1])
print("Percent of Patients who Did `Not Showed Up` => {:.2f}%".format(noshow*100))

"""Around 2/3 (old 80%) of the patients are coming for the visit after an appointment and around 1/3 (old 20%) are skipping their appointments.

Let's continue our wrangling.
"""

# we have more sparse records the more we go into greater awaiting time
# let's group awaiting days into awaiting days categoriess
AwaitingDaysLabels = pd.Series([
    0,#'A: Same day',
    1,#'B: 1-2 days',
    2,#'C: 3-7 days',
    3,#'D: 8-31 days',
    4#'E: 32+ days'
    ])

datasetRaw['AwaitingDaysCategory'] = pd.cut(
    datasetRaw.AwaitingDays, bins = [-1, 0, 2, 7, 31, 999],
    labels = AwaitingDaysLabels,
    )

# let's group single age years into 10 years groups
datasetRaw['AgeGroup'] = (
    datasetRaw.Age.apply(lambda x: min(int(x / 10) , 9))
    )

# the values greater than 1 for handicap are very rares, so let's just record if one patient has some form of handicap or not
datasetRaw['HasHandicap'] = (datasetRaw.Handicap > 0)

# drop old columns
datasetRaw = datasetRaw.drop(['Age'],axis=1)
datasetRaw = datasetRaw.drop(['AwaitingDays'],axis=1)
datasetRaw = datasetRaw.drop(['Handicap'],axis=1)

# let's transform some fields into more useful types, for example if the patient didnt show up the true becomes 1, and have the gender as binary.
datasetRaw['NoShow'] = pd.get_dummies(datasetRaw['NoShow'])['Yes']
datasetRaw['Gender'] = pd.get_dummies(datasetRaw['Gender'])['M']
datasetRaw['HasHandicap'] = pd.get_dummies(datasetRaw['HasHandicap'])[True]


datasetRaw.sample(n=10)

datasetRaw.head()
#salviamo il dataset pulito e normalizzato in un nuovo file KaggleV2-May-2016_pulito.csv
#datasetRaw.to_csv("gdrive/My Drive/KaggleV2-May-2016_pulito.csv")

#utilizziamo questo dataset per la rete neurale 
#datasetRaw_neuralNetwork = datasetRaw
#datasetRaw_neuralNetwork.dtypes

"""# Feature Selection Step-Forward"""

# Feature Selection with the step forward approach

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import LabelEncoder

le=LabelEncoder()

neighbourhood_labels = le.fit_transform(datasetRaw['Neighbourhood'])

# Useful only to view the changes
neighbourhood_mappings = {index: label for index, label in enumerate(le.classes_)}
print(neighbourhood_mappings)

datasetRaw1 = datasetRaw
datasetRaw1['Neighbourhood']=neighbourhood_labels

X = datasetRaw1.drop(['NoShow'],axis=1)  # Features
y = datasetRaw1['NoShow']  # Labels

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33)

print('Training dataset shape:', X_train.shape, y_train.shape)
print('Testing dataset shape:', X_test.shape, y_test.shape)

from mlxtend.feature_selection import SequentialFeatureSelector as sfs
from sklearn.linear_model import LogisticRegression,LogisticRegressionCV

logmodel = LogisticRegression(penalty="l1",solver="liblinear")

# Build step forward feature selection
# default cv is 5
# k_features=best to have the best subset without limiting ourselves to a fixed number
sfs1 = sfs(logmodel,k_features='best',forward=True,floating=False,scoring='accuracy',verbose=2,cv=3)

# Perform Step Forward Feature Selection
sfs1 = sfs1.fit(X_train, y_train)



#see info on the subsets
sfs1.subsets_

# our best subset features indexes
sfs1.k_feature_idx_

# our best subset features names
sfs1.k_feature_names_



# our best subset "accuracy" score (not strictly an accuracy, but it is close)
sfs1.k_score_

"""(old 80/20 balance

The feauture selection isn't working nicely, we have the same score on all the intermediate selections and the result its just the first k features. We can also already foresee now a problem with this dataset, some models just train them to predict yes on all the test datapoints, they can reach easyly an accuracy of 80% because the dataset is very unbalanced with its 20/80 proportions.

)

I want to try now the same feauture selection but with scaled feautures.

# Feature Selection Step-Forward with Scaled Features
"""

# import StandardScaler from Scikit learn
from sklearn.preprocessing import StandardScaler

# create StandardScaler object
scaler = StandardScaler()

# fit scaler to features
scaler.fit(datasetRaw.drop(['NoShow'],axis=1))
StandardScaler()
# use .transform() to transform features to scaled version
scaled_features = scaler.transform(datasetRaw.drop('NoShow',axis=1))
datasetScaled = pd.DataFrame(scaled_features)
datasetScaled.head()
#there are 12 columns because we dropped the noShow column

Xsc = datasetScaled  # Features
ysc = datasetRaw['NoShow']  # Labels

# Train/test split
X_trainsc, X_testsc, y_trainsc, y_testsc = train_test_split(Xsc,ysc,test_size=0.3)

print('Training dataset shape:', X_trainsc.shape, y_trainsc.shape)
print('Testing dataset shape:', X_testsc.shape, y_testsc.shape)

logmodel = LogisticRegression(penalty="l1",solver="liblinear")

# Build step forward feature selection
sfs2 = sfs(logmodel,k_features='best',forward=True,floating=False,verbose=2,n_jobs=-1,scoring='accuracy',cv=3)

# Perform Step Forward Feature Selection
sfs2 = sfs2.fit(X_trainsc, y_trainsc)

#see info on the subsets
sfs2.subsets_

# our best subset features indexes
sfs2.k_feature_idx_

# our best subset features names, a bit useless with our scaled features
sfs2.k_feature_names_

# our best subset "accuracy" score (not strictly an accuracy, but it is close)
sfs2.k_score_

"""In differents and sequentials runnings of the step-forward feature selection we aren't getting the same results, so i want to try to run an all-subsets feature selection to universally find the best subset (if i could get a middle subsets and a nice accuracy too, that would be great).

# Feature Selection All-Subsets
"""

#Feature Selection with the all-subsets approach
"""
from sklearn.model_selection import train_test_split

from sklearn.preprocessing import LabelEncoder

le=LabelEncoder()

neighbourhood_labels = le.fit_transform(datasetRaw['Neighbourhood'])

# Useful only to view the changes
neighbourhood_mappings = {index: label for index, label in enumerate(le.classes_)}
print(neighbourhood_mappings)

datasetRaw1 = datasetRaw
datasetRaw1['Neighbourhood']=neighbourhood_labels

X = datasetRaw1.drop(['NoShow'],axis=1)  # Features
y = datasetRaw1['NoShow']  # Labels

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

print('Training dataset shape:', X_train.shape, y_train.shape)
print('Testing dataset shape:', X_test.shape, y_test.shape)

from mlxtend.feature_selection import ExhaustiveFeatureSelector as efs

logmodel = LogisticRegression(max_iter=10000)

# Build step forward feature selection
# default cv is 5
efs1 = efs(logmodel,min_features=1,max_features=12,cv=3)

# Perform Step Forward Feature Selection
efs1 = efs1.fit(X_train, y_train)
"""

# see info on the subsets
#efs1.subsets_

# our best subset features indexes
#efs1.best_idx_

# our best subset features names, a bit useless with our scaled features
#efs1.best_feature_names_

# our best subset "accuracy" score (not strictly an accuracy, but it is close)
#efs1.best_score_

"""The all-subsets feature selection method isn't satisfactory:

1 is taking too long to compute, obsvly

2 the score "accuracy" of the best subest is a lot close to the ones from the step forward approach

3 every time we compute we have differents results here too

# Modeling attempts

# With Feature Selection

# Logistic Regression

We cannot use the OneHot coded Neighbourood with this logistic model if we want to exploit any kind of feature selection.
"""

# Generate the new subsets based on the selected features
# Note that the transform call is equivalent to
# X_train[:, sfs1.k_feature_idx_]

X_train_sfs = sfs1.transform(X_train)
X_test_sfs = sfs1.transform(X_test)

# Instantiate model
logmodelfs = LogisticRegression(max_iter=1000)
logmodelfscv = LogisticRegressionCV(max_iter=1000,cv=3) # default 5 folds

# Fit the estimator using the new feature subset
# and make a prediction on the test data
# Train model
logmodelfs.fit(X_train_sfs,y_train)
logmodelfscv.fit(X_train_sfs,y_train)

# Get predictions
log_predfs = logmodelfs.predict(X_test_sfs)
log_predfscv = logmodelfscv.predict(X_test_sfs)

#Import scikit-learn metrics module for accuracy calculation
from sklearn.metrics import classification_report,confusion_matrix

print(classification_report(y_test,log_predfs))

print("Confusion matrix:\n",confusion_matrix(y_test, log_predfs))

print(classification_report(y_test,log_predfscv))

print("Confusion matrixcv:\n",confusion_matrix(y_test, log_predfscv))

"""# Without Feature Selection

We aren't getting outstanding results with the feature subsets, so let's try to train with all the features.
"""

# import Decision Tree and Random Forest models
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

"""# DecisionTreeClassifier"""

X = datasetRaw.drop(['NoShow'],axis=1)
y = datasetRaw['NoShow']  # Labels

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test

# prepare and train the model
decTree_clf = DecisionTreeClassifier(random_state=0)
decTree_clf.fit(X_train, y_train)

# print Feature Importance
print("Feature Importance:\n")
for name, importance in zip(X.columns, np.sort(decTree_clf.feature_importances_)[::-1]):
    print("{} -- {:.2f}".format(name, importance))

# let's see the tree score
decTree_clf.score(X_test, y_test)

"""# RandomForestClassifier"""

# prepare and train the model

randForest_clf = RandomForestClassifier(random_state=0)
randForest_clf.fit(X_train, y_train)

# print Feature Importance
print("Feature Importance:\n")
for name, importance in zip(X.columns, np.sort(randForest_clf.feature_importances_)[::-1]):
    print("{} -- {:.2f}".format(name, importance))

# let's see the forest score
randForest_clf.score(X_test, y_test)

"""Both the simple DecisionTree and the RandomForest models work best without scaling the values of the dataset.

#AdaBoost
"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.datasets import make_classification
X = datasetRaw.drop(['NoShow'],axis=1) # Features
y = datasetRaw['NoShow']  # Labels
# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test
clf = AdaBoostClassifier(n_estimators=200,random_state=0,base_estimator=DecisionTreeClassifier(max_depth=2))
clf.fit(X_train, y_train)

#let's see the AdaBoost score
clf.score(X_test,y_test)

"""This is our best result for now.

# Coding OneHot Neighbourhood

Let's code the neighbourood field to One-Hot before trying to train a model.
"""

# get dummy variables for neighbourhood
neighbourhoods = pd.get_dummies(datasetRaw['Neighbourhood'])
# join dummy neighbourhood columns and drop string neighbourhood column
datasetRaw_2 = datasetRaw.join(neighbourhoods).drop('Neighbourhood',axis=1)

datasetRaw_2.sample(n=10)

"""# Scaling"""

# Let's standardise the variables with the one-hot neighbourhood to prepare data for future modelling.

# create StandardScaler object
scaler = StandardScaler()

# fit scaler to features
scaler.fit(datasetRaw_2.drop(['NoShow'],axis=1))
StandardScaler()
# use .transform() to transform features to scaled version
scaled_features = scaler.transform(datasetRaw_2.drop('NoShow',axis=1))
datasetScaled = pd.DataFrame(scaled_features)
datasetScaled.head()

"""# Logistic Regression Model both without and with Cross Validation"""

X = datasetScaled  # Features
y = datasetRaw['NoShow']  # Labels

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test

riskClasses=["High Probability NoShow","Low Probability NoShow","Low Probability Show","High Probability Show"]

# Function to see what risk Class is coded with the x probability of 0 of NoShow
# x is the probability that the datapoint results in a Positive Show
# high probability results in high probability of the label being 0, that is a negative NoShow (so a positive Show)
def gimmeRiskClass(x):
  if(x<0.25):
    return riskClasses[0]
  if(x<0.5):
    return riskClasses[1]
  if(x<0.75):
    return riskClasses[2]
  else: return riskClasses[3]

# Instantiate model
logmodel = LogisticRegression()
logmodelcv = LogisticRegressionCV(cv=3) # default 5 folds

# Train model
logmodel.fit(X_train,y_train)
logmodelcv.fit(X_train,y_train)

# Get predictions
log_pred = logmodel.predict(X_test)
log_pred_proba = logmodel.predict_proba(X_test)
log_predcv = logmodelcv.predict(X_test)

#Import scikit-learn metrics module for accuracy calculation
from sklearn.metrics import classification_report,confusion_matrix

print("Without Cross Validation:\n")

print(classification_report(y_test,log_pred))

print("Confusion matrix:\n",confusion_matrix(y_test, log_pred))

# Computing exact count of risk Classes
ShowLabels = [0,0,0,0]
for x in log_pred_proba:
  if(x[0]<0.25):
    ShowLabels[0] = ShowLabels[0]+1
  else:
    if(x[0]<0.5):
      ShowLabels[1] = ShowLabels[1]+1
    else:
      if(x[0]<0.75):
        ShowLabels[2] = ShowLabels[2]+1
      else: ShowLabels[3] = ShowLabels[3]+1

print('\n')
print(ShowLabels)
print('\nWe predict '+str(ShowLabels[0])+' '+riskClasses[0])
print('We predict '+str(ShowLabels[1])+' '+riskClasses[1])
print('We predict '+str(ShowLabels[2])+' '+riskClasses[2])
print('We predict '+str(ShowLabels[3])+' '+riskClasses[3]+'\n\n')

print("With Cross Validation:\n")

print(classification_report(y_test,log_predcv))

print("Confusion matrixcv:\n",confusion_matrix(y_test, log_predcv))

"""(old 80/10

We can reach an accuracy of 80%, but we can note something, that the logistic regression with cross validation is about the same accuracy but it could be worse because it predicts almost all of the test set as a negative NoShow, while the logistic without cross validation can predict successfully some more positive NoShow. Also as we could say earlier we can note that this model likes to train himself to predict always a negative NoShow and reach a no-brain 80% accuracy because the dataset is very unbalanced.

)

# Rete Neurale

Load in memory MPLClassifier algorithm and accuray_store to compare prevision with the correct answers.
"""

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

X = datasetScaled # Features
y = datasetRaw['NoShow']  # Labels

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test

"""We set two hidden layers each one with 100 nodes"""

model = MLPClassifier(hidden_layer_sizes=[100,100], verbose=True, max_iter=200, tol=0.000001)
history = model.fit(X_train, y_train)
loss = history.loss_
iter = history.max_iter

import matplotlib.pyplot as plt
def plot_history(history):
  #print(model.loss_curve_)
  a = np.arange(1,history.max_iter+1,1)
  #print(a)
  plt.figure()
  plt.xlabel('Iter')
  plt.ylabel('Loss Function ')
  plt.plot(a, np.array(history.loss_curve_),label='Train Loss')
  plt.legend()
  plt.ylim([0.0 ,0.5])

plot_history(model)

"""Save prediction in p_train and p_test respectivley"""

p_train = model.predict(X_train) 
p_test = model.predict(X_test)

acc_train = accuracy_score(y_train, p_train) 
acc_test = accuracy_score(y_test, p_test)

print("acc. train : ", acc_train) 
print("acc. test : ", acc_test)